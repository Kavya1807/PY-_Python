# -*- coding: utf-8 -*-
"""
Created on Sat May  8 14:36:52 2021


"""
import glob
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import matplotlib.pyplot as plt
import keras as k
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

data= pd.read_csv('ANN.csv')
data.columns
print(data)
print(data.shape)
print(data.ndim)
print(data.describe())
sns.heatmap(data.isnull(),yticklabels = False, cmap='viridis');
data = data.dropna(axis = 0)
print(data.shape)
for column in data.columns:
    if data[column].dtype == np.number:
        continue
    data[column] = LabelEncoder().fit_transform(data[column])
print(data.shape)
print(data.head())
print(data.describe())
sns.heatmap(data.isnull(),yticklabels = False, cmap='viridis');
# splitting dataset into dependent and independent

X = data.drop(['class'],axis=1)
Y = data['class']
#Feature Scaling
#min-max scaler method scales the data set so that all the input features lie between 0 and 1
x_scaler = MinMaxScaler()
x_scaler.fit(X)
column_names = X.columns
X[column_names] = x_scaler.transform(X)
print(X)
#split the data into 80% training and 20% testing and shuffle

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2 ,shuffle = True)
#build model

model = Sequential()

#first layer
model.add(Dense(256,input_dim= len(X.columns),kernel_initializer= k.initializers.random_normal(seed= 13),activation= 'relu'))

#second layer
model.add(Dense (1, activation = 'hard_sigmoid'))
#compiling the model
#opt = adam(lr=0.001, decay=1e-6)

model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

#train the model

history = model.fit(X_train, Y_train, epochs = 250, batch_size = X_train.shape[0])
#save the model

model.save('ckd.model');
#visualie model loss and accuracy

plt.plot(history.history['accuracy'],label='Accuracy')
plt.plot(history.history['loss'],label='Loss')
plt.title('Model Loss and Accuracy')
plt.ylabel('Accuracy and Loss')
plt.xlabel('Epochs')   
plt.legend();
print("shape of training data: ",X_train.shape)
print("shape of test data: ",X_test.shape)
#show actual and predicted value
pred = model.predict(X_test)
pred = [1 if y>=0.5 else 0 for y in pred]

print('Original:  {0}'.format(",".join(str(x) for x in Y_test)))
print('Predicted: {0}'.format(",".join(str(x) for x in pred)))
X = data.iloc[:,:-1]
y = data['class']

logreg = LogisticRegression(max_iter = 10000)
X_train, X_test, y_train, y_test = train_test_split(X,y, stratify = y, shuffle = True)
logreg.fit(X_train,y_train)
test_pred = logreg.predict(X_test)
train_pred = logreg.predict(X_train)

print('Train Accuracy: ', accuracy_score(y_train, train_pred))
print('Test Accuracy: ', accuracy_score(y_test, test_pred))
tn, fp, fn, tp = confusion_matrix(y_test, test_pred).ravel()

print(f'True Neg: {tn}')
print(f'False Pos: {fp}')
print(f'False Neg: {fn}')
print(f'True Pos: {tp}')
print(classification_report(y_test,test_pred))
